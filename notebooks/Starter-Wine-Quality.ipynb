{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import _____ # TODO: Import the correct function \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset using pandas\n",
    "df = pd.read_csv(\"_______\") # replace with correct filename\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check for missing values in the dataset\n",
    "df.______() # Fill in the correct function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.____() # TODO: add function to show basic statistical details like mean, min, max, and standard deviation for each feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert 'quality' into a binar classification: 1 for good quality (>=7), 0 otherwise\n",
    "df['quality'] = df['quality'].apply(lambda x: ____ if x >= ____ else ____) # Fill in missing logic\n",
    "df.rename(columns={'_____': 'good-quality'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.countplot(x='good-quality', data=df)\n",
    "plt.xlabel('Good Quality')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Good vs Bad Quality Wines')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of coorelation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correlation between features and the target variable\n",
    "# Hint: whats the target variable\n",
    "df.corr()['______'][:-1].sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize correlation matrix using a heatmap. \n",
    "# This step does not require changing anything. Just think\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze relation between features using scatterplots\n",
    "\n",
    "fig, ax = plt.subplots(2,4,figsize=(20,20))\n",
    "sns.scatterplot(x = 'fixed acidity', y = 'citric acid', hue = 'good-quality', data = df, ax=ax[0,0])\n",
    "sns.scatterplot(x = 'volatile acidity', y = 'citric acid', hue = 'good-quality', data = df, ax=ax[0,1])\n",
    "sns.scatterplot(x = 'free sulfur dioxide', y = 'total sulfur dioxide', hue = 'good-quality', data = df, ax=ax[0,2])\n",
    "sns.scatterplot(x = 'fixed acidity', y = 'density', hue = 'good-quality', data = df, ax=ax[0,3])\n",
    "sns.scatterplot(x = 'fixed acidity', y = 'pH', hue = 'good-quality', data = df, ax=ax[1,0])\n",
    "sns.scatterplot(x = 'citric acid', y = 'pH', hue = 'good-quality', data = df, ax=ax[1,1])\n",
    "sns.scatterplot(x = 'chlorides', y = 'sulphates', hue = 'good-quality', data = df, ax=ax[1,2])\n",
    "sns.scatterplot(x = 'residual sugar', y = 'alcohol', hue = 'good-quality', data = df, ax=ax[1,3])\n",
    "\n",
    "# Question: Which features seem to have a strong relationship with wine quality?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "X_train, X_test, y_train, y_test = # Fill in the missing part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the Logistic Regression model using the training data\n",
    "lr.fit(___, ___) # Fillin the correct arguments\n",
    "\n",
    "# TODO: Evaluate model performance on the training data\n",
    "lr.score(___, ___) # FIll in the correct arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use trained model to make predictions on the test set\n",
    "lr_pred = lr.predict(___) # Fill missing args\n",
    "\n",
    "# TODO: Calculate accuracy of model\n",
    "accuracy_score(y_test, lr_pred) # Fill in correct args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='rbf')\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the Logistic Regression model using the training data\n",
    "\n",
    "# TODO: Evaluate model performance on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the trained SVM model to make predictions on the test set\n",
    "\n",
    "\n",
    "# TODO: Calculate the accuracy of SVM model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "dtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "dtree.fit(X_train, y_train)\n",
    "dtree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use trained dtree to make prediction\n",
    "tr_pred = dtree.predict(___) # Fill args\n",
    "\n",
    "# TODO: Calculate accuracy of model\n",
    "accuracy_score(___, ____) # Fill args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the K-Nearest Neighbors model using training data\n",
    "knn.___(___, ___) # Fill function and args\n",
    "\n",
    "# TODO: Evaluate performance\n",
    "knn.score(___, ___) # Fill args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use trained KNN model to predict\n",
    "kn_pred = knn.predict(____) # Fill Args\n",
    "\n",
    "# TODO: Calc accuracy\n",
    "accuracy_score(____, ___) # Fill args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: no need to change anything. analyze the performance of the LR model using the confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, lr_pred), annot=True, cmap='Blues')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evalutate model using multiply metrics\n",
    "# Fill in args for all\n",
    "\n",
    "print('Logistic Regression Model Accuracy: ', accuracy_score(___, ____))\n",
    "# add precision and recall metrics \n",
    "print('Logistic Regression Model f1 score: ', metrics.f1_score(____, ____))\n",
    "print('Logistic Regression Model MAE: ', metrics.mean_absolute_error(____, ____))\n",
    "print('Logistic Regression Model RMSE: ', np.sqrt(metrics.mean_squared_error(____, ____)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: no change. take a second to analyze the performance using the matrix\n",
    "sns.heatmap(confusion_matrix(y_test, sv_pred), annot=True, cmap='Reds')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.title('Confusion Matrix for Support Vector Machine')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate model using metric\n",
    "# Fill in args\n",
    "\n",
    "print('Support Vector Machine Model Accuracy: ', accuracy_score(___, ___))\n",
    "# add precision and recall metrics \n",
    "print('Support Vector Machine Model f1 score: ', metrics.f1_score(___, ___))\n",
    "print('Support Vector Machine Model MAE: ', metrics.mean_absolute_error(___, ___))\n",
    "print('Support Vector Machine Model RMSE: ', np.sqrt(metrics.mean_squared_error(___, ___)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the Decision tree model\n",
    "# Follow the structure of the previous model evaluations (Logistic Regression, SVM)\n",
    "# - Generate the confusion matrix heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the Decision tree model\n",
    "# Follow the structure of the previous model evaluations (Logistic Regression, SVM)\n",
    "# - Calculate and print Accuracy, F1 Score, MAE, and RMSE\n",
    "# - Format the print statements similarly to the other models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the K-Nearest Neighbors (KNN) model\n",
    "# Follow the structure of the previous model evaluations (Logistic Regression, SVM, and Decision Tree)\n",
    "# - Generate the confusion matrix heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the K-Nearest Neighbors (KNN) model\n",
    "# Follow the structure of the previous model evaluations (Logistic Regression, SVM, and Decision Tree)\n",
    "# - Calculate and print Accuracy, F1 Score, MAE, and RMSE\n",
    "# - Format the print statements similarly to the other models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Logistic Regression', 'Support Vector Machine', 'Decision Tree', 'K-Nearest Neighbors']\n",
    "# TODO: Add the accuracy for KNN based on evaluation\n",
    "accuracy = [accuracy_score(y_test, lr_pred), accuracy_score(y_test, sv_pred), accuracy_score(y_test, tr_pred), ______] # Fill in accuracy for KNN\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=models, y=accuracy)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Write conclusion based on results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
